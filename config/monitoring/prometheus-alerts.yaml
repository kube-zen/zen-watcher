apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: zen-watcher-alerts
  namespace: zen-system
  labels:
    app: zen-watcher
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
  - name: zen-watcher-health
    interval: 30s
    rules:
    - alert: ZenWatcherDown
      expr: zen_watcher_health_status == 0
      for: 2m
      labels:
        severity: critical
        component: zen-watcher
      annotations:
        summary: "Zen Watcher is unhealthy"
        runbook_url: "https://github.com/your-org/zen-watcher/wiki/Runbook-ZenWatcherDown"
    
    - alert: ZenWatcherNotReady
      expr: zen_watcher_readiness_status == 0
      for: 5m
      labels:
        severity: warning
        component: zen-watcher
      annotations:
        summary: "Zen Watcher is not ready"
    
    - alert: ZenWatcherHighMemoryUsage
      expr: process_resident_memory_bytes{job="zen-watcher"} / 1024 / 1024 > 512
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
      annotations:
        summary: "Zen Watcher high memory usage"
    
    - alert: ZenWatcherHighCPUUsage
      expr: rate(process_cpu_seconds_total{job="zen-watcher"}[5m]) > 0.9
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
      annotations:
        summary: "Zen Watcher high CPU usage"
    
    - alert: ZenWatcherTooManyGoroutines
      expr: zen_watcher_goroutines > 1000
      for: 5m
      labels:
        severity: warning
        component: zen-watcher
      annotations:
        summary: "Zen Watcher has too many goroutines"

  - name: zen-watcher-events
    interval: 30s
    rules:
    - alert: HighCriticalEventRate
      expr: rate(zen_watcher_events_total{severity="HIGH"}[5m]) > 10
      for: 5m
      labels:
        severity: critical
        component: zen-watcher
        category: events
      annotations:
        summary: "High rate of critical security events"
        description: "zen-watcher is creating {{ $value }} HIGH severity events per second"
    
    - alert: HighEventProcessingLatency
      expr: histogram_quantile(0.99, sum(rate(zen_watcher_event_processing_duration_seconds_bucket[5m])) by (le, source)) > 5
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
        category: performance
      annotations:
        summary: "High event processing latency"
        description: "p99 event processing latency is {{ $value }}s for {{ $labels.source }}"

  - name: zen-watcher-informers
    interval: 30s
    rules:
    - alert: InformerCacheNotSynced
      expr: zen_watcher_informer_cache_synced == 0
      for: 5m
      labels:
        severity: warning
        component: zen-watcher
        category: informers
      annotations:
        summary: "Informer cache not synced"
        description: "Informer cache for {{ $labels.resource }} is not synced"
    
    - alert: InformerCacheSyncFailure
      expr: zen_watcher_informer_cache_synced == 0
      for: 10m
      labels:
        severity: critical
        component: zen-watcher
        category: informers
      annotations:
        summary: "Informer cache sync failure"
        description: "Informer cache for {{ $labels.resource }} has been unsynced for more than 10 minutes"

  - name: zen-watcher-operations
    interval: 30s
    rules:
    - alert: HighCRDOperationFailureRate
      expr: rate(zen_watcher_crd_operations_total{status="failure"}[5m]) / rate(zen_watcher_crd_operations_total[5m]) > 0.05
      for: 5m
      labels:
        severity: critical
        component: zen-watcher
        category: operations
      annotations:
        summary: "High CRD operation failure rate"
        runbook_url: "https://github.com/your-org/zen-watcher/wiki/Runbook-CRDOperationFailures"
    
    - alert: SlowEventProcessing
      expr: histogram_quantile(0.95, sum(rate(zen_watcher_event_processing_duration_seconds_bucket[5m])) by (le, source)) > 5
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
        category: performance
      annotations:
        summary: "Event processing is slow"
        description: "p95 event processing latency is {{ $value }}s for {{ $labels.source }}"
    
    - alert: HTTPErrorRate
      expr: rate(zen_watcher_http_requests_total{status=~"5.."}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: zen-watcher
        category: operations
      annotations:
        summary: "High HTTP error rate"
    
    - alert: SlowHTTPRequests
      expr: histogram_quantile(0.95, sum(rate(zen_watcher_http_request_duration_seconds_bucket[5m])) by (le, endpoint)) > 1
      for: 10m
      labels:
        severity: info
        component: zen-watcher
        category: performance
      annotations:
        summary: "Slow HTTP requests"

  - name: zen-watcher-kubernetes
    interval: 30s
    rules:
    - alert: KubernetesAPIErrors
      expr: rate(zen_watcher_k8s_api_requests_total{status=~"error|failure"}[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: zen-watcher
        category: kubernetes
      annotations:
        summary: "Kubernetes API errors"
    
    - alert: SlowKubernetesAPIRequests
      expr: histogram_quantile(0.95, sum(rate(zen_watcher_k8s_api_request_duration_seconds_bucket[5m])) by (le, resource)) > 10
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
        category: performance
      annotations:
        summary: "Slow Kubernetes API requests"

  - name: zen-watcher-slo
    interval: 1m
    rules:
    # Event Processing SLO: 99% of events should be processed within 5 seconds
    - alert: EventProcessingSLOViolation
      expr: |
        (
          histogram_quantile(0.99, sum(rate(zen_watcher_event_processing_duration_seconds_bucket[5m])) by (le, source))
          > 5
        )
      for: 10m
      labels:
        severity: warning
        component: zen-watcher
        category: slo
      annotations:
        summary: "Event processing SLO violation"
        description: "p99 event processing latency is {{ $value }}s for {{ $labels.source }} (SLO: 5s)"
    
    # Availability SLO: 99.9% uptime (based on up metric from Prometheus)
    - alert: AvailabilitySLOViolation
      expr: |
        (
          avg_over_time(up{job="zen-watcher"}[5m])
        ) < 0.999
      for: 5m
      labels:
        severity: critical
        component: zen-watcher
        category: slo
      annotations:
        summary: "Availability SLO violation"
        description: "zen-watcher availability is below 99.9% SLO"


